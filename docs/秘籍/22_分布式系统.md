# 分布式系统

### ==1.什么是分布式？为什么要用分布式？什么是微服务架构？==

**1）什么是分布式？**

分布式就是将整个工程按照业务进行拆分为多个项目。每个项目部署在不同的节点上，部署在不同结点上的系统通过网络交互来完成协同工作的系统。

**SOA 面向服务的架构**（Service Oriented Architecture），也就是把工程按照业务逻辑拆分成**服务层、表现层**两个工程。服务层中包含业务逻辑，只需要对外提供服务即可。表现层只需要处理和页面的交互，业务逻辑都是调用服务层的服务来实现。SOA架构中有两个主要角色：**服务提供者（Provider）和服务使用者（Consumer）**。

**分布式或者说 SOA 分布式**重要的就是**面向服务**，说简单的分布式就是我们**把整个系统拆分成不同的服务然后将这些服务放在不同的服务器上减轻单体服务的压力提高并发量和性能**。

**2）为什么要用分布式？**

从开发角度来讲单体应用的代码都集中在一起，而分布式系统的代码根据业务被拆分。所以，每个团队可以负责一个服务的开发，这样**提升了开发效率**。另外，代码根据业务拆分之后**更加便于维护和扩展**。

将系统拆分成分布式之后不光**便于系统扩展和维护**，更能**提高整个系统的性能**。

**3）什么是“微服务架构”呢？**

简单的说，微服务架构就是将一个完整的应用从数据存储开始**垂直**拆分成多个不同的服务，每个服务都能独立部署、独立维护、独立扩展，服务与服务间通过诸如**RESTful API**的方式互相调用。

==**4）微服务架构的优点和缺点**==

优点：

- **逻辑清晰**
- **简化部署**
- **可扩展**
- **灵活组合**
- **技术异构**
- **高可靠**

缺点：

- **复杂度高**
- **运维复杂**
- **影响性能**

### ==2.分布式系统一致性session如何实现==

将用户的 session+Token 信息保存到 **Redis 分布式缓存**当中。在鑫课堂项目中就用到了这种方法。

**1）session复制**

将 session 在多个系统之间复制，但是由于将session进行复制需要占用过多的内网带宽并且存储内容较多，让系统内存占用率过高。

**2）session前端存储**

将 session 保存到 cookie 中，但这种方式有安全性问题。

**3）session沾滞**

配置 nginx 让同一 ip 的请求只访问到同一台机器，但还是存在性能以及session存储在内存的问题。

**4）session后端集中存储**

使用 redis 来将 session 进行集中式存储，还可以持久化保存，当请求过多时也可以进行水平扩展。推荐使用。

### ==3.分布式事务实现？==



### ==4.Spring Eureka注册中心实现原理？==

微服务架构中最核心的部分是服务治理，服务治理最基础的组件是注册中心。

注册中心是分布式开发的核心组件之一，而eureka是spring cloud推荐的注册中心实现。Eureka提供了完整的**Service Registry**和**Service Discovery**实现。

![](https://pic4.zhimg.com/v2-8a0cfb3dc04641a44d468e91fcc3514a_r.jpg)

- **Eureka Server**：提供服务注册和发现，多个Eureka Server之间会同步数据，做到状态一致（最终一致性）
- **Service Provider**：服务提供方，将自身服务注册到Eureka，从而使服务消费方能够找到
- **Service Consumer**：服务消费方，从Eureka获取注册服务列表，从而能够消费服务

从 CAP 理论看，Eureka 是一个 AP 系统，优先保证可用性A 和 分区容错性P，不保证强一致性C，只保证最终一致性，因此在架构中设计了较多缓存。

**自我保护机制**

自我保护机制主要在Eureka Client和Eureka Server之间存在网络分区的情况下发挥保护作用，在服务器端和客户端都有对应实现。假设在某种特定的情况下（如网络故障）, Eureka Client和Eureka Server无法进行通信，此时Eureka Client无法向Eureka Server发起注册和续约请求，Eureka Server中就可能因注册表中的服务实例租约出现大量过期而面临被剔除的危险，然而此时的Eureka Client可能是处于健康状态的（可接受服务访问），如果直接将注册表中大量过期的服务实例租约剔除显然是不合理的，自我保护机制提高了eureka的服务可用性。

**服务发现原理**

eureka server可以集群部署，多个节点之间会进行（**异步方式**）数据同步，保证数据最终一致性，Eureka Server作为一个开箱即用的服务注册中心，提供的功能包括：**服务注册、接收服务心跳、服务剔除、服务下线**等。需要注意的是，Eureka Server同时也是一个Eureka Client，在不禁止Eureka Server的客户端行为时，它会向它配置文件中的其他Eureka Server进行拉取注册表、服务注册和发送心跳等操作。

eureka server端通过`appName`和`instanceInfoId`来唯一区分一个服务实例，服务实例信息是保存在哪里呢？其实就是一个Map中：

```javascript
// 第一层的key是appName，第二层的key是instanceInfoId
private final ConcurrentHashMap<String, Map<String, Lease<InstanceInfo>>> registry 
    = new ConcurrentHashMap<String, Map<String, Lease<InstanceInfo>>>();
```

**服务注册**

Service Provider启动时会将服务信息（InstanceInfo）发送给eureka server，eureka server接收到之后会写入registry中，服务注册默认过期时间`DEFAULT_DURATION_IN_SECS = 90`秒。InstanceInfo写入到本地registry之后，然后同步给其他peer节点，对应方法`com.netflix.eureka.registry.PeerAwareInstanceRegistryImpl#replicateToPeers`。

**写入本地registry**

服务信息（InstanceInfo）保存在Lease中，写入本地registry对应方法`com.netflix.eureka.registry.PeerAwareInstanceRegistryImpl#register`，Lease统一保存在内存的`ConcurrentHashMap`中，在服务注册过程中，首先加个读锁，然后从registry中判断该Lease是否已存在，如果已存在则比较`lastDirtyTimestamp`时间戳，取二者最大的服务信息，避免发生数据覆盖。使用InstanceInfo创建一个新的InstanceInfo：

```javascript
if (existingLastDirtyTimestamp > registrationLastDirtyTimestamp) {
    // 已存在Lease则比较时间戳，取二者最大值
    registrant = existingLease.getHolder();
}
Lease<InstanceInfo> lease = new Lease<InstanceInfo>(registrant, leaseDuration);
if (existingLease != null) {
    // 已存在Lease则取上次up时间戳
    lease.setServiceUpTimestamp(existingLease.getServiceUpTimestamp());
}

public Lease(T r, int durationInSecs) {
    holder = r;
    registrationTimestamp = System.currentTimeMillis(); // 当前时间
    lastUpdateTimestamp = registrationTimestamp;
    duration = (durationInSecs * 1000);
}
```

不知道小伙伴看了上述方法的代码有没有这样的疑问？

> 通过读锁并且 `registry` 的读取和写入不是原子的，那么在并发时其实是有可能发生数据覆盖的，如果发生数据覆盖岂不是有问题了！猛一看会以为脏数据不就是有问题么？换个角度想，脏数据就一定有问题么？ 其实针对这个问题，eureka的处理方式是没有问题的，该方法并发时，针对InstanceInfo Lease的构造，二者的信息是基本一致的，因为registrationTimestamp取的就是当前时间，所以并并发的数据不会产生问题。

**同步给其他peer**

InstanceInfo写入到本地registry之后，然后同步给其他peer节点，对应方法`com.netflix.eureka.registry.PeerAwareInstanceRegistryImpl#replicateToPeers`。如果当前节点接收到的InstanceInfo本身就是另一个节点同步来的，则不会继续同步给其他节点，避免形成“广播效应”；InstanceInfo同步时会排除当前节点。

InstanceInfo的状态有依以下几种：`Heartbeat, Register, Cancel, StatusUpdate, DeleteStatusOverride`，默认情况下同步操作时批量异步执行的，同步请求首先缓存到Map中，key为`requestType+appName+id`，然后由发送线程将请求发送到peer节点。

> Peer之间的状态是采用异步的方式同步的，所以不保证节点间的状态一定是一致的，不过基本能保证最终状态是一致的。结合服务发现的场景，实际上也并不需要节点间的状态强一致。在一段时间内（比如30秒），节点A比节点B多一个服务实例或少一个服务实例，在业务上也是完全可以接受的（Service Consumer侧一般也会实现错误重试和负载均衡机制）。所以按照CAP理论，Eureka的选择就是放弃C，选择AP。 如果同步过程中，出现了异常怎么办呢，这时会根据异常信息做对应的处理，如果是读取超时或者网络连接异常，则稍后重试；如果其他异常则打印错误日志不再后续处理。

**服务续约**

Renew（服务续约）操作由Service Provider定期调用，类似于heartbeat。主要是用来告诉Eureka Server Service Provider还活着，避免服务被剔除掉。renew接口实现方式和register基本一致：首先更新自身状态，再同步到其它Peer，服务续约也就是把过期时间设置为当前时间加上duration的值。

> 注意：服务注册如果InstanceInfo不存在则加入，存在则更新；而服务预约只是进行更新，如果InstanceInfo不存在直接返回false。

**服务下线**

Cancel（服务下线）一般在Service Provider shutdown的时候调用，用来把自身的服务从Eureka Server中删除，以防客户端调用不存在的服务，eureka从本地”删除“（设置为删除状态）之后会同步给其他peer，对应方法`com.netflix.eureka.registry.PeerAwareInstanceRegistryImpl#cancel`。

**服务失效剔除**

Eureka Server中有一个EvictionTask，用于检查服务是否失效。Eviction（失效服务剔除）用来定期（默认为每60秒）在Eureka Server检测失效的服务，检测标准就是超过一定时间没有Renew的服务。默认失效时间为90秒，也就是如果有服务超过90秒没有向Eureka Server发起Renew请求的话，就会被当做失效服务剔除掉。失效时间可以通过`eureka.instance.leaseExpirationDurationInSeconds`进行配置，定期扫描时间可以通过`eureka.server.evictionIntervalTimerInMs`进行配置。

服务剔除#evict方法中有很多限制，都是为了保证Eureka Server的可用性：比如自我保护时期不能进行服务剔除操作、过期操作是分批进行、服务剔除是随机逐个剔除，剔除均匀分布在所有应用中，防止在同一时间内同一服务集群中的服务全部过期被剔除，以致大量剔除发生时，在未进行自我保护前促使了程序的崩溃。

### ==5.微服务故障恢复机制==

- **服务降级**：出现故障时执行服务降级策略来保证整体系统可用。
- **变更管理**：当对服务进行修改时……例如发布代码的新版本或者改变一些配置，则总会有可能引起故障或者引入新的错误。为了应对变更带来的问题，你可以实施变更策略管理并且实现其自动回滚。比如，当部署新的代码或者修改配置时，应该分步将这些变更部署到服务实例群中的部分实例中，并且进行监控，如果发现关键指标出现问题则能自动进行回滚。
- **健康检查和负载均衡**：持续从实例中收集健康信息，并且设置负载均衡的路由，让其只指向健康的实例组件。
- **自我修复**：通过外部系统监控实例的状态，当服务出现故障一段时间后则会重启服务。
- **故障转移缓存（Failover Caching）**：失效转移缓存通常使用两个不同的过期日期：其中更短的日期指示在正常情况下能使用缓存的时间，而更长的一个日期则指示在故障失效的时候，能使用缓存中的数据时长。
- **重试逻辑（Retry Logic）**：微服务系统重试可能会触发多个其他请求或重试操作，并导致级联效应。为减少重试带来的影响，你应该减少重试的数量，并使用指数退避算法（exponential backoff algorithm）来持续增加重试之间的延迟时间，直到达到最大限制。
- **限流器和负载开关（Rate Limiters and Load Shedders）**：限流是指在一段时间内，定义某个客户或应用可以接收或处理多少个请求的技术。使用负载开关可以确保对于关键的事务总能提供足够的资源保障，它为高优先级的请求保留一些资源，并且不允许低优先级的事务去占用这些资源。
- **快速且单独失效（Fail Fast and Independently）**：服务可以快速、单独地失效。为了在服务层面隔离故障，我们可以使用隔板模式（bulkhead pattern）。可以使用基于操作的成功/失败统计次数的熔断模式，而不是使用超时。
- **舱壁模式（Bulkheads）**：在工业领域中，常使用舱壁将划分为几个部分，以便在有某部分船体发生破裂时，其他部分依然能密封安然无恙。
- **断路器（Circuit Breakers）**：可以使用断路器来处理错误，而不是使用小型和特定基于事务的静态超时机制。断路器以现实世界的电子元件命名，因为它们的行为是都是相同的。你可以保护资源，并通过使用断路器协助它们进行恢。断路器在分布式系统中非常有用，因为重复的故障可能会导致雪球效应，并使整个系统崩溃。当在短时间内多次发生指定类型的错误，断路器会开启。开启的断路器可以拒绝接下来更多的请求 – 就像防止真实的电子流动一样。断路器通常在一定时间后关闭，以便为底层服务提供足够的空间来恢复。
- **故障测试（Testing for Failures）**：应该持续地测试系统的常见问题，以确保你的服务可各类故障环境下运行。你应经常测试故障，以让你的团队对可能发生的事故有所准备。

### ==6.分布式锁==

分布式锁可以使用 **zookeeper** 或者 **redis 自身提供的分布式锁**以及**Mysql 乐观锁**等。

redis 的分布式锁支持如下的特性：

- **互斥性**：在任意一个时刻，只有一个客户端持有锁。
- **无死锁**：即便持有锁的客户端崩溃或者其他意外事件，锁仍然可以被获取。
- **容错**：只要大部分Redis节点都活着，客户端就可以获取和释放锁

每次写之前都需要判读一下当前的这个 value 的**时间戳**是否比缓存里的 value 的时间戳更新，如果更新则可以写，如果更旧则不能用旧的数据覆盖新的数据。

Redis 分布式锁主要利用 Redis 的 **setnx** （如果不存在则更新）命令。

SETNX（**SET if Not eXist**）的使用方式为：**SETNX key value**，只在键key不存在的情况下，将键key的值设置为value，若键key存在，则SETNX不做任何动作。

SETNX在设置成功时返回，设置失败时返回0。当要获取锁时，直接使用SETNX获取锁，当要释放锁时，使用DEL命令删除掉对应的键key即可。

- 加锁命令：**SETNX key value**，当键不存在时，对键进行设置操作并返回成功，否则返回失败。KEY 是锁的唯一标识，一般按业务来决定命名。
- 解锁命令：**DEL key**，通过删除键值对释放锁，以便其他线程可以通过 SETNX 命令来获取锁。
- 锁超时：**EXPIRE key timeout**, 设置 key 的超时时间，以保证即使锁没有被显式释放，锁也可以在一定时间后自动释放，避免资源被永远锁住。

上面这种方案有一个致命问题，就是**某个线程在获取锁之后由于某些异常因素（比如宕机）而不能正常的执行解锁操作，那么这个锁就永远释放不掉了**。

为此，我们可以为这个锁加上一个**超时时间**，第一时间我们会联想到Redis的**EXPIRE**命令（EXPIRE key seconds）。

但是这里我们不能使用EXPIRE来实现分布式锁，因为它与SETNX一起是两个操作，在这两个操作之间可能会发生异常，从而还是达不到预期的结果，示例如下：

```java
// STEP 1
SETNX key value
// 若在这里（STEP1和STEP2之间）程序突然崩溃，则无法设置过期时间，将有可能无法释放锁
// STEP 2
EXPIRE key expireTime
```

对此，正确的姿势应该是使用 **“SET key value [EX seconds] [PX milliseconds] [NX|XX]”** 这个命令，也就是set key的时候就直接指定对应的超时时间。

修改之后的方案看上去很完美，但实际上还是会有问题。

试想一下，某线程A获取了锁并且设置了过期时间为10s，然后在执行业务逻辑的时候耗费了15s，此时线程A获取的锁早已被Redis的过期机制自动释放了。

在线程A获取锁并经过10s之后，该锁可能已经被其它线程获取到了。当线程A执行完业务逻辑准备解锁（DEL key）的时候，有可能删除掉的是其它线程已经获取到的锁。

所以最好的方式是**在解锁时判断锁是否是自己的**，我们可以在设置key的时候将value设置为一个**唯一值uniqueValue**（可以是随机值、UUID、或者机器号+线程号的组合、签名等）。

当解锁时，也就是**删除key**的时候**先判断一下key对应的value是否等于先前设置的值，如果相等才能删除key**，伪代码示例如下：

```java
if uniqueKey == GET(key) {
    DEL key
}
```

这里我们一眼就可以看出问题来：**GET和DEL是两个分开的操作，在GET执行之后且在DEL执行之前的间隙是可能会发生异常的**。

如果我们只要保证解锁的代码是**原子性**的就能解决问题了。

这里我们引入了一种新的方式，就是**Lua脚本**，示例如下：

```java
if redis.call("get",KEYS[1]) == ARGV[1] then
    return redis.call("del",KEYS[1])
else
    return 0
end
```

其中ARGV[1]表示设置key时指定的唯一值。

由于Lua脚本的**原子性**，在Redis执行该脚本的过程中，其他客户端的命令**都需要等待该Lua脚本执行完才能执行**。一个基于 lua 脚本的实现中间件就是**redisson**。

表面来看，这个方法似乎很管用，但是这里存在一个问题：**在我们的系统架构里存在一个单点故障，如果Redis的master节点宕机了怎么办呢？**

有人可能会说：加一个slave节点！在master宕机时用slave就行了！

但是其实这个方案明显是不可行的，因为Redis的复制是**异步**的。是为了保证 **AP** （可用性、分区容忍性）而忽略 C（一致性，只保证最终一致性）。

举例来说：

1. 线程A在master节点拿到了锁。
2. master节点在把A创建的key写入slave之前宕机了。
3. slave变成了master节点。
4. 线程B也得到了和A还持有的相同的锁。（因为原来的slave里面还没有A持有锁的信息）

当然，在某些场景下这个方案没有什么问题，比如业务模型允许同时持有锁的情况，那么使用这种方案也未尝不可。

举例说明，某个服务有2个服务实例A和B，初始情况下A获取了锁然后对资源进行操作（可以假设这个操作很耗费资源），B没有获取到锁而不执行任何操作，此时B可以看做是A的热备。

当A出现异常时，B可以“转正”，当锁出现异常时，比如Redis master宕机，那么B可能会同时持有锁并且对资源进行操作，如果操作的结果是幂等的（或者其它情况），那么也可以使用这种方案。

这里引入分布式锁可以让服务在正常情况下避免重复计算而造成资源的浪费。

为了应对这种情况，antriez提出了**Redlock算法**。

Redlock算法的主要思想是：**假设我们有N个Redis master节点，这些节点都是完全独立的，我们可以运用前面的方案来对前面单个的Redis master节点来获取锁和解锁**

如果我们总体上能**在合理的范围内或者N/2+1个锁，那么我们就可以认为成功获得了锁，反之则没有获取锁**（可类比Quorum模型）。

虽然Redlock的原理很好理解，但是其内部的实现细节很是复杂，要考虑很多因素

具体内容可以参考：[https://redis.io/topics/distlock](https://link.zhihu.com/?target=https%3A//redis.io/topics/distlock)。

Redlock算法也并非是“银弹”，他除了条件有点苛刻外，其算法本身也被质疑。

使用Redis分布式锁并不能做到万无一失。一般而言，Redis分布式锁的优势在于**性能**，而如果要考虑到**可靠性**，那么**Zookeeper**这类的组件会比Redis要高。因为 **Zookeeper**是保证**CP**（一致性、分区容忍性）忽略**A**（可用性），会等数据在集群中复制成功之后才返回。

当然，在合适的环境下使用基于数据库实现的分布式锁会更合适。

不过就以可靠性而言，没有任何组件是完全可靠的，程序员的价值不仅仅在于表象地如何灵活运用这些组件，而在于如何基于这些不可靠的组件构建一个可靠的系统。

还是那句老话，选择何种方案，合适最重要。

各种分布式锁实现方式的优缺点：

**Mysql**小结

- 适用场景: Mysql分布式锁一般适用于**资源存在数据库**，如果数据库存在比如订单，那么可以直接对这条数据加行锁，不需要我们上面多的繁琐的步骤，比如一个订单，那么我们可以用select * from order_table where id = 'xxx' for update进行加**行锁**，那么其他的事务就不能对其进行修改。
- 优点:理解起来简单，不需要维护额外的第三方中间件(比如Redis,Zk)。
- 缺点:虽然容易理解但是实现起来较为繁琐，需要自己考虑锁超时，加事务等等。性能局限于数据库，一般对比缓存来说性能较低。对于高并发的场景并不是很适合。

**ZK**小结

- 优点:ZK可以**不需要关心锁超时时间**，实现起来有现成的第三方包，比较方便，并且支持读写锁，ZK获取锁**会按照加锁的顺序**，所以其是公平锁。对于高可用利用ZK集群进行保证。
- 缺点:ZK需要**额外维护**，增加维护成本，性能和Mysql相差不大，依然比较差。并且需要开发人员了解ZK是什么。

**Redis**小结

- 优点:对于Redis实现简单，性能对比ZK和Mysql较好。如果不需要特别复杂的要求，那么自己就可以利用setNx进行实现，如果自己需要复杂的需求的话那么可以利用或者借鉴**Redission**。对于一些要求比较严格的场景来说的话可以使用**RedLock**。
- 缺点:需要维护Redis集群，如果要实现RedLock那么需要维护更多的集群。

### ==7.Spring Cloud Zuul介绍？为什么要使用Zuul？如何实现？==

**1）Zuul介绍**

[Zuul](https://github.com/Netflix/zuul) Netflix 开发的一款提供**动态路由、监控、弹性、安全**的**网关服务**。网关的作用相当于一个过虑器、拦截器，它可以拦截多个系统的请求。 

Spring Cloud Zuul 是整合 Netﬂix 公司的 Zuul 开源项目实现的微服务网关，它实现了**请求路由、负载均衡、校验过虑**等功能。 

服务网关是在微服务前边设置一道屏障，请求先到服务网关，网关会对请求进行**过滤、校验、路由**等处理。

有了服务网关可以提高微服务的**安全性**，网关校验请求的合法性，请求不合法将被拦截，拒绝访问。 

Zuul与Nginx怎么配合使用？ 

Zuul与Nginx在实际项目中需要配合使用，Nginx的作用是**反向代理、负载均衡**，Zuul的作用是**保障微服务的安全访问，拦截微服务请求，校验合法性及负载均衡**。

**2）为什么要使用Zuul？**

Zuul使用了一系列不同类型的过滤器，使我们能够快速灵活地将功能应用到服务中。这些过滤器帮助我们执行以下功能：

- **身份验证和安全性** ： 识别每个资源的身份验证需求，并拒绝不满足它们的请求
- **监控** ： 在边缘跟踪有意义的数据和统计数据，以便给我们一个准确的生产视图
- **动态路由** ： 动态路由请求到不同的后端集群
- **压力测试** ： 逐渐增加集群的流量，以评估性能
- **限流** ： 为每种请求类型分配容量，并丢弃超过限制的请求
- **静态响应处理** ： 直接在边缘构建一些响应，而不是将它们转发到内部集群

**3）Zuul是如何工作的？**

Zuul的核心业务逻辑是过滤器。它们能够执行非常大范围的操作，并且可以在请求-响应生命周期的不同阶段运行。如下图所示：

![](https://images2018.cnblogs.com/blog/874963/201808/874963-20180823175304773-255386381.png)

- **Inbound Filters** ： 路由到 Origin 之前执行，可以用于身份验证、路由和装饰请求
- **Endpoint Filters** ： 可用于返回静态响应，否则内置的ProxyEndpoint过滤器将请求路由到Origin
- **Outbound Filters** ： 从Origin那里获取响应后执行，可以用于度量、装饰用户的响应或添加自定义header

有两种类型的过滤器：sync 和 async。因为Zuul是运行在一个事件循环之上的，因此从来不要在过滤中阻塞。如果你非要阻塞，可以在一个异步过滤器中这样做，并且在一个单独的线程池上运行，否则可以使用同步过滤器。

过滤器是Zuul的核心功能。它们负责应用程序的业务逻辑，可以执行各种任务。

- **Type** ： 通常定义过滤器应用在哪个阶段
- **Async** ： 定义过滤器是同步还是异步
- **Execution Order** ： 执行顺序
- **Criteria** ： 过滤器执行的条件
- **Action** ： 如果条件满足，过滤器执行的动作

Zuul提供了一个动态读取、编译和运行这些过滤器的框架。过滤器之间不直接通信，而是通过每个请求特有的RequestContext共享状态。

### ==8.Spring Hystrix 介绍？如何实现熔断、降级？==

**1)Hystrix介绍**

 **hystrix**的出现即为解决**雪崩**效应，它通过四个方面的机制来解决这个问题

- **隔离**（线程池隔离和信号量隔离）：限制调用分布式服务的资源使用，某一个调用的服务出现问题不会影响其他服务调用。
- 优雅的**降级**机制：超时降级、资源不足时(线程或信号量)降级，降级后可以配合降级接口返回托底数据。
- **熔断**：当失败率达到阀值自动触发降级(如因网络故障/超时造成的失败率高)，熔断器触发的快速失败会进行快速恢复。
- **缓存**：提供了请求缓存、请求合并实现。
- 支持实时监控、报警、控制

**2）Hystrix-隔离**

![](https://upload-images.jianshu.io/upload_images/6302559-697d22f49dab0ec2.png)

Hystrix的隔离主要是为每个依赖组件提供一个隔离的线程环境，提供两种模式的隔离：

- **线程池隔离模式**：使用一个线程池来存储当前的请求，线程池对请求作处理，设置任务返回处理超时时间，堆积的请求堆积入线程池队列。这种方式需要为每个依赖的服务申请线程池，有一定的资源消耗，好处是可以应对突发流量（流量洪峰来临时，处理不完可将数据存储到线程池队里慢慢处理）
- **信号量隔离模式**：使用一个原子计数器（或信号量）来记录当前有多少个线程在运行，请求来先判断计数器的数值，若超过设置的最大线程个数则丢弃改类型的新请求，若不超过则执行计数操作请求来计数器+1，请求返回计数器-1。这种方式是严格的控制线程且立即返回模式，无法应对突发流量（流量洪峰来临时，处理的线程超过数量，其他的请求会直接返回，不继续去请求依赖的服务）

|          |        线程池隔离        |             信号量隔离 |
| -------- | :----------------------: | ---------------------: |
| 线程     |   与调用线程不相同线程   |         与调用线程相同 |
| 开销     | 排队、调度、上下文开销等 |     无线程切换，开销低 |
| 异步     |           支持           |                 不支持 |
| 并发支持 |  支持（最大线程池大小）  | 支持（最大信号量上限） |

**3)Hystrix-熔断器**

![](https://upload-images.jianshu.io/upload_images/6302559-ec9d98e87f874fe5.png)



Hystrix的熔断器其实可以理解为就是一个**统计中心，统计一定时间窗口内访问次数，成功次数，失败次数等数值**判定是否发生**熔断**。发生电路熔断的过程如下：

- 假设电路上的音量达到一定阈值（HystrixCommandProperties.circuitBreakerRequestVolumeThreshold）
- 并假设错误百分比超过阈值错误百分比（HystrixCommandProperties.circuitBreakerErrorThresholdPercentage）
- 然后断路器从CLOSED转换到OPEN。
- 它是开放的，它使所有针对该断路器的请求短路。
- 经过一段时间（HystrixCommandProperties.circuitBreakerSleepWindowInMilliseconds），下一个单个请求是通过（这是HALF-OPEN状态）。 如果请求失败，断路器将在睡眠窗口持续时间内返回到OPEN状态。 如果请求成功，断路器将转换到CLOSED，逻辑1.重新接管。

**4)Hystrix工作流程**

![](https://upload-images.jianshu.io/upload_images/6302559-a6b6eb713e12dd69.png)

1. 构建一个HystrixCommand或者HystrixObservableCommand 对象。
2. 执行Command
3. 响应是否有缓存？如果为该命令启用请求缓存，并且如果缓存中对该请求的响应可用，则此缓存响应将立即以“可观察”的形式返回。
4. 熔断器是否打开？如果电路打开（或“跳闸”），则Hystrix将不会执行该命令，但会将流程路由到（8）获取回退。如果电路关闭，则流程进行到（5）以检查是否有可用于运行命令的容量。
5. 线程池/队列/信号量是否已经满负载？如果与命令相关联的线程池和队列（或信号量，如果不在线程中运行）已满，则Hystrix将不会执行该命令，但将立即将流程路由到（8）获取回退。
6. 执行真正的命令部分，HystrixObservableCommand.construct() 或者 HystrixCommand.run()，在这里Hystrix通过您为此目的编写的方法调用对依赖关系的请求。如果run或construct方法超出了命令的超时值，则该线程将抛出一个TimeoutException, 在这种情况下，Hystrix将响应通过8进行路由。获取Fallback，如果该方法不取消/中断，它会丢弃最终返回值run（）或construct（）方法。
7. 计算Circuit 的健康，Hystrix向断路器报告成功，失败，拒绝和超时，该断路器维护了一系列的计算统计数据组。它使用这些统计信息来确定电路何时“跳闸”，此时短路任何后续请求直到恢复时间过去，在首次检查某些健康检查之后，它再次关闭电路。
8. 获取Fallback，当命令执行失败时，Hystrix试图恢复到你的回退：当construct或run抛出异常时[6]，当命令由于电路断开而短路时[4]，当命令的线程池和队列或信号量处于容量[5]，或者当命令超过其超时长度时[6]。
9. 返回成功的响应，如果 Hystrix command成功，如果Hystrix命令成功，它将以Observable的形式返回对呼叫者的响应或响应。

