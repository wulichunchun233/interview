# 并发

### 0.有没有在项目中实际使用多线程？

有

在鑫课堂项目中为了实现选课功能的**分布式事务**实现，采用了消息队列来实现分布式事务的AP（可用性和分区容忍性）特性，并满足最终一致性。

为了将多个**选课消息**从**订单服务**发送到**学习服务**中进行处理，需要用到**定时任务**处理的方法，即在指定的时间间隔内重复扫描**任务消息表**并将其发送到消息队列中去。

因为项目是使用 Spring 全家桶来实现的，因此定时任务采用 Spring Task 来实现任务调度。但是 Spring Task 默认是**串行执行的**，无法实现将多个任务**并行执行**，降低了系统运行的性能。因此就使用**线程池**来实现**多线程任务调度**。

创建线程池的方法使用 Executor 的 ThreadPoolTaskScheduler。使用 @Configuration @EnableScheduling 注解来实现多线程的使用。

```java
    /**
     * 创建一个线程池
     * @return
     */
    @Bean
    public ThreadPoolTaskScheduler taskScheduler() {
        ThreadPoolTaskScheduler scheduler = new ThreadPoolTaskScheduler();
        // 初始化线程池
        scheduler.initialize();
        // 线程池容量
        scheduler.setPoolSize(corePoolSize);
        return scheduler;
    }
```

### ==1.并发编程三要素，Java内存模型介绍，指令重排，happens-before原则==

**1）并发编程三要素**

- **原子性**：当一个线程对数据进行操作的时候另一个线程会等待该线程执行完之后才可以继续执行自己的任务。也就是一个线程在执行的过程中必须全部一次性独立执行完，等执行完之后才可以继续执行下一个线程的任务。

- **有序性**：对于代码，还存在的一个问题就是**指令重排序**，也就是编译器和指令器，有的时候会为了提高代码执行效率，会将指令重新排序。而具备有序性则不会发生指令重排，保证代码不会出现问题。

- **可见性**：一个线程对数据的修改另一个线程可以立即看到。

**2）Java内存模型介绍**

java 的内存模型需要主要掌握六个原子操作：**read、load、use、assign、store、write**，两个位置：**主内存、线程工作内存**，多个线程。

线程工作内存中保存：线程栈、CPU缓存、寄存器。

具体来说：

创建的所有资源变量都是保存在**主内存**中的，当一个线程对其进行访问的话是需要需要将其从**主内存**中读取加载到自己的**工作内存**中去的，这里就设计到**read、load**操作，当该变量的值到达线程的工作内存之后线程需要使用**use**操作来从自己的**工作内存**中使用该变量。

当对该变量操作完成之后也是需要将对应的变量在写入内存的，首先使用**assign**来将修改之后的变量写入到**工作内存**中，然后在使用**store、write**来将变量从工作内存中写入到**主内存**中去。

以上才是完整的单个线程的 java 内存模型，假如存在多个线程共同访问内存中的数据的话就会出现各种各样的问题，此时就需要使用**多线程并发**的知识来解决。

也就是保证**原子性、有序性、可见性**。

**3）指令重排以及 happens-before 原则**

**指令重排**是指java编译器和指令器为了提高代码的执行效率会将程序中的代码的顺序进行重新排序，导致出现多线程访问的问题。

java中有一个**happens-before**原则：编译器、指令器可能对代码进行重排序，但是要遵守一定的规则，即 happens-before 原则。只要符合 happens-before 原则，则不能胡乱排序，如果不符合这些规则的话，那么就可以自己排序。

**happens-before原则**包括：

- **程序次序**规则：一个线程内，按照代码顺序，书写在前面的操作先发生于书写在后面的操作。
- **锁定**规则：一个 unlock 操作先发生于后面一个锁的 lock 操作。
- **volatile 变量**规则：对一个变量的写操作先发生于后面的读操作。
- **传递**规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C。
- **线程启动**规则：Thread对象的start方法先行发生于此线程的每一个动作。
- **线程中断**规则：对线程 interrupt 的方法先行发生于被中断线程的代码检测到中断事件的发生。
- **线程终结**规则：线程中所有的操作都先行发生于线程的终止检测。
- **对象终结规**则：一个对象的初始化完成先行发生于他的 finalize 方法的开始。

这些规则保证了在一些特殊情况下，不允许编译器、指令器不能对代码进行重排，必须保证代码的**有序性**。

### 2.多线程的好处（为什么要使用多线程？）

**1）发挥多核CPU的优势**

多线程，可以真正发挥出多核CPU的优势来，达到充分利用CPU的目的，采用多线程的方式去同时完成几件事情而不互相干扰。

**2）防止阻塞**

从程序运行效率的角度来看，单核CPU不但不会发挥出多线程的优势，反而会因为在单核CPU上运行多线程导致线程上下文的切换，而降低程序整体的效率。但是单核CPU我们还是要应用多线程，就是为了防止阻塞。试想，如果单核CPU使用单线程，那么只要这个线程阻塞了，比方说远程读取某个数据吧，对端迟迟未返回又没有设置超时时间，那么你的整个程序在数据返回回来之前就停止运行了。多线程可以防止这个问题，多条线程同时运行，哪怕一条线程的代码执行读取数据阻塞，也不会影响其它任务的执行。

**3）便于建模**

这是另外一个没有这么明显的优点了。假设有一个大的任务A，单线程编程，那么就要考虑很多，建立整个程序模型比较麻烦。但是如果把这个大的任务A分解成几个小任务，任务B、任务C、任务D，分别建立程序模型，并通过多线程分别运行这几个任务，那就简单很多了。

### ==3.创建线程的方法？==

- **继承Thread类**创建线程类
  - 定义Thread类的子类，并重写该类的**run方法**，该run方法的方法体就代表了线程要完成的任务。因此把run()方法称为执行体。
  - 创建Thread子类的实例，即创建了线程对象。
  - 调用线程对象的**start()**方法来启动该线程。
- 实现**Runnable接口**创建线程类
  - 定义runnable接口的实现类，并重写该接口的**run()方法**，该run()方法的方法体同样是该线程的线程执行体。
  - 创建 Runnable实现类的实例，并依此实例作为Thread的target来创建Thread对象，该Thread对象才是真正的线程对象。
  - 调用线程对象的**start()**方法来启动该线程。
- 实现**Callable接口**创建线程
  - 创建Callable接口的实现类，并实现**call()**方法，该call()方法将作为线程执行体，并且有返回值。
  - 创建Callable实现类的实例，使用FutureTask类来包装Callable对象，该FutureTask对象封装了该Callable对象的call()方法的返回值。
  - 使用FutureTask对象作为Thread对象的target创建并启动新线程。
  - 调用FutureTask对象的get()方法来获得子线程执行结束后的返回值。
  - **Future就是对于具体的Runnable或者Callable任务的执行结果进行取消、查询是否完成、获取结果**。必要时可以通过**get方法**获取执行结果，该方法会阻塞直到任务返回结果。
  - FutureTask一个可取消的异步计算，**FutureTask 实现了Future的基本方法**，提供 start cancel 操作，可以查询计算是否已经完成，并且可以获取计算的结果。结果只可以在计算完成之后获取，get方法会阻塞当计算没有完成的时候，一旦计算已经完成，那么计算就不能再次启动或是取消。

创建线程的三种方式的对比：

1、采用**实现Runnable、Callable接口**的方式创见多线程时

**优势是**：

- 线程类只是实现了Runnable接口或Callable接口，还可以继承其他类。

- 在这种方式下，多个线程可以共享同一个target对象，所以非常适合多个相同线程来处理同一份资源的情况，从而可以将CPU、代码和数据分开，形成清晰的模型，较好地体现了面向对象的思想。

**劣势是**：

- 编程稍微复杂，如果要访问当前线程，则必须使用Thread.currentThread()方法。

2、使用**继承Thread类**的方式创建多线程时

**优势是**：

- 编写简单，如果需要访问当前线程，则无需使用Thread.currentThread()方法，直接使用this即可获得当前线程。

**劣势是**：

- 线程类已经继承了Thread类，所以不能再继承其他父类。

### ==4.线程的生命周期和状态?==

Java 线程在运行的生命周期中的指定时刻只可能处于下面 6 种不同状态的其中一个状态：

![7](/Users/wx/project/interview/docs/秘籍/images/7.png)

线程在生命周期中并不是固定处于某一个状态而是随着代码的执行在不同状态之间切换。Java 线程状态变迁如下图所示:

![8](/Users/wx/project/interview/docs/秘籍/images/8.png)

线程创建之后它将处于 **NEW（新建）** 状态，调用 `start()` 方法后开始运行，线程这时候处于 **READY（可运行）** 状态。可运行状态的线程获得了 CPU 时间片（timeslice）后就处于 **RUNNING（运行）** 状态（操作系统隐藏 Java 虚拟机（JVM）中的 RUNNABLE 和 RUNNING 状态，它只能看到 RUNNABLE 状态）当线程执行 `wait()`方法之后，线程进入 **WAITING（等待）** 状态。进入等待状态的线程需要依靠其他线程的通知才能够返回到运行状态，而 **TIME_WAITING(超时等待)** 状态相当于在等待状态的基础上增加了超时限制，比如通过 `sleep（long millis）`方法或 `wait（long millis）`方法可以将 Java 线程置于 TIMED WAITING 状态。当超时时间到达后 Java 线程将会返回到 RUNNABLE 状态。当线程调用同步方法时，在没有获取到锁的情况下，线程将会进入到 **BLOCKED（阻塞）** 状态。线程在执行 Runnable 的`run()`方法之后将会进入到 **TERMINATED（终止）** 状态。

### 5.Java中的并发工具类

- **CountDownLatch**允许一个或多个线程等待其他线程完成操作。`CountDownLatch`传入一个N当做计数器，每次执行countDown的时候N就会减1，`CountDownLatch`的await方法就会阻塞当前线程，直到N变成零。countDown可以是一个线程中的N个步骤或者是N个线程。
- **CyclicBarrier ** 同步屏障。让一组线程到达一个屏障（或者是同步点）的时候被阻塞，直到最后一个线程到达屏障，屏障才会打开，所有的线程继续往下执行。
- **Semaphore**（信号量）用来控制同时访问特定资源的线程数量，通过协调各个线程，以保证合理的使用公共资源。
- **Exchanger**是个用于线程间协作的工具类，用于线程之间的数据交换。它提供一个同步点，在这个同步点，两个线程可以交换彼此的数据。第一个线程先执行`exchange()`方法，第二个线程也执行`exchange()`方法，当两个线程同时到达同步点，这两个线程就可以交换数据。如果一个线程一直没有执行`exchange()`方法，那么会一直等下去，如果担心特殊情况，可以使用`exchange(V v,longtimeout, TimeUnit unit)`设置最大等待时间。

CyclicBarrier 和 CountDownLatch 的区别：

1. CountDownLatch 的计数器只能使用一次
2. CyclicBarrier 的计数器可以使用 reset() 方法重置，因此 CyclicBarrier 可以实现更加复杂的功能。

### ==6.synchronized 关键字详解==

**1）synchronized关键字用途**

synchronized关键字解决的是**多个线程之间访问资源的同步性**，synchronized关键字可以保证**被它修饰的方法或者代码块在任意时刻只能有一个线程执行**。

synchronized可以根据其支持的加锁对象分为：**类锁、对象锁、方法锁、静态方法锁**。（一般来说给对象进行加锁）

**2）synchronized关键字最主要的三种使用方式**

- **修饰实例方法:** 作用于当前**对象实例**加锁，进入同步代码前要获得当前对象实例的锁；
- **修饰静态方法:** 也就是给**当前类**加锁，会作用于类的所有对象实例，因为静态成员不属于任何一个实例对象，是类成员（ static 表明这是该类的一个静态资源，不管new了多少个对象，只有一份）。所以如果一个线程 A 调用一个实例对象的非静态 synchronized 方法，而线程 B 需要调用这个实例对象所属类的静态 synchronized 方法，是允许的，不会发生互斥现象，**因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例对象锁**。
- **修饰代码块:** 指定加锁对象，对**给定对象**加锁，进入同步代码库前要获得给定对象的锁。

**总结：** synchronized 关键字加到 static 静态方法和 synchronized(class)代码块上都是是给 Class 类上锁。synchronized 关键字加到实例方法上是给对象实例上锁。尽量不要使用 synchronized(String a) 因为JVM中，字符串常量池具有缓存功能！

**3）synchronized关键字的底层原理**

synchronized 关键字底层原理属于 **JVM** 层面。

synchronized **同步语句块**的实现使用的是 **monitorenter** 和 **monitorexit** 指令，其中 **monitorenter** 指令指向同步代码块的**开始位置**，**monitorexit** 指令则指明同步代码块的**结束位置**。

每个**对象**都有一个关联的**monitor**，比如一个对象实例就有一个 **monitor**，一个类的class对象也有一个**monitor**，如果要对这个对象加锁，那么就必须对这个对象关联的**monitor**进行加锁。

**monitor** 里面有一个**计数器**，初始值为**0**，表示没有人对其进行加锁。当有线程对对象进行加锁的话就将对象的 **monitor** 计数器进行**加一**。然后还可以继续对其进行加锁，将计数器**继续加一**，也就是支持**可重用锁**。当该线程执行结束之后就将对应的计数器**减一**，释放锁资源。

当有另外的一个线程到来的时候，就先判断该对象的**monitor**中的计数器是否为0，假如不为0表示已经有线程对其加锁还未释放，此时该线程就**阻塞**等待其释放锁。当另外的线程释放锁资源之后该线程就可以对其进行加锁，也就是将对象对应的**monitor**的线程计数器进行加一。

### ==7.volatile 关键字详解==

**1）volatile应用**

得先介绍一下 Java 的**内存模型**：**read、load、use、assign、store、write，主内存、工作内存，多线程，原子性、有序性、可见性**。

（假如面试官没有问上面的概念，则需要自己去讲）

volatile主要用来解决**可见性和有序性**，在有些罕见的条件之下，也可以有限的保证原子性，但**不能用来保证原子性**的，保证原子性还是需要使用 sychronized 等锁机制来加锁的，因此不可以在并发环境下去保证共享数据的安全。

**2）volatile如何保证可见性和有序性**

- **可见性**保证：

当变量被 volatile 修饰之后，当线程再次从内存中读取该变量并对其修饰之后会**强制**的让其将更新后的变量从**工作内存**中写入到**主内存**中。同时让其他线程自己的**工作内存**中保存的该变量的副本**失效**。这样其他线程想要再次获取该变量的值的时候就只能从**主内存**中去读取。这样就保证了**可见性**。

- **有序性**保证：

基于 **happens-before** 原则，被 **volatile** 修饰的变量的**写操作**会先发生于**读操作**，且**不可以进行指令重排**。这样就保证了**有序性**。

**3）volatile保证可见性和有序性底层原理**

- **可见性**底层实现：

volatile 保证**可见性**底层是基于 **lock指令+MESI（M：修改、E：独占、S：共享、I：失效）缓存一致性协议**。对 volatile 修饰的变量，执行写操作的话，JVM会发送一条**lock指令**给CPU，CPU在计算完之后会立即将这个值写回主内存，同时因为有 MESI 缓存一致性协议，所以各个 CPU 都会对总线进行**嗅探**，来查看自己本地缓存中的数据是否被别人修改。如果发现被人修改了某个缓存的数据，那么CPU会将自己本地缓存中的数据过期掉，然后这个CPU上执行的线程在读取这个变量的时候就会从主存中重新加载最新的数据了。

- **有序性**底层实现：

volatile 保证**有序性**底层是基于**内存屏障**机制来保证编译器、指令器不能对代码进行**指令重排**。具体来说会在代码中涉及到 volatile 变量的地方加上对应的**内存屏障**：load屏障、store屏障等。这样内存屏障会在操作 volatile 变量的前后加入对应的内存屏障来保证指令不会被重排。

### ==8.CAS 技术详解==

**1）CAS概念介绍**

**CAS（Compare and Swap），即比较并替换**，是一种实现并发算法时常用到的技术。

像 synchronized 这种独占锁属于**悲观锁**，它是在假设**一定会发生冲突的**，那么加锁恰好有用。

除此之外，还有**乐观锁**，**乐观锁**的含义就是**假设没有发生冲突，那么我正好可以进行某项操作，如果要是发生冲突呢，那我就重试直到成功**，**乐观锁最常见的就是CAS。**

比如并发包中的一些原子类：==**AtomicInteger** 等底层就是基于 CAS 来进行实现的。==

**2）CAS实现原理**

CAS需要有3个操作数：**内存地址V，旧的预期值A，即将要更新的目标值B**。

CAS指令执行时，**当且仅当内存地址V的值与预期值A相等时，将内存地址V的值修改为B，否则就什么都不做**。整个比较并替换的操作是一个**原子操作**，在底层的硬件级别保证。

**3）CAS存在的问题**

CAS虽然很高效的解决了原子操作问题，但是CAS仍然存在三大问题。

1. **循环时间长开销很大。**我们可以看到getAndAddInt方法执行时，如果CAS失败，会一直进行尝试。如果CAS长时间一直不成功，可能会给CPU带来很大的开销。
2. **只能保证一个共享变量的原子操作。**当对一个共享变量执行操作时，我们可以使用循环CAS的方式来保证原子操作，但是对多个共享变量操作时，循环CAS就无法保证操作的原子性，这个时候就可以用锁来保证原子性。
3. **ABA问题。**如果内存地址V初次读取的值是A，如果在这段期间它的值曾经被改成了B，后来又被改回为A，那CAS操作就会误认为它从来没有被改变过。这个漏洞称为CAS操作的“ABA”问题。Java并发包为了解决这个问题，提供了一个带有标记的原子引用类 **AtomicStampedReference**，它可以通过控制变量值的**版本**来保证CAS的正确性。因此，在使用CAS前要考虑清楚“ABA”问题是否会影响程序并发的正确性，如果需要解决ABA问题，改用传统的互斥同步可能会比原子类更高效。

### ==9.AQS 类介绍==

**1）AQS概念介绍**

AQS的全称为（Abstract Queued Synchronizer，抽象队列同步器）。

AQS是一个用来**构建锁和同步器**的框架，使用AQS能简单且高效地构造出应用广泛的大量的同步器，比如**ReentrantLock，CountDownLatch**

**2）AQS实现原理**

AQS核心思想是，如果被请求的**共享资源空闲**，则将当前请求资源的线程设置为有效的**工作线程**，并且将共享资源设置为**锁定状态**。如果被请求的共享资源被占用，那么就需要一套**线程阻塞等待以及被唤醒时锁分配的机制**，这个机制AQS是用**CLH队列**（CLH队列是一个虚拟的双向队列，AQS是将每条请求共享资源的线程封装成一个CLH锁队列的一个Node来实现锁的分配。）锁实现的，即将暂时获取不到锁的线程加入到队列中。

![42](/Users/wx/project/interview/docs/秘籍/images/42.png)

**3）那么AQS是怎么来判断一个资源被占用呢？**

**CAS + state计数器**

具体来说，**AQS** 会在自身设置一个 **state** 计数器，初始值为0。当一个线程获取资源的时候首先使用**CAS**机制来判断当前的 **state** 是否为0，假如为0则成功获取到资源，并将对应的 **state** 设置为 1。这样其他的线程到来的时候使用**CAS**机制发现 **state** 为1就获取不到资源，然后就进入 **CLH** 队列中进行排队。直到资源被释放，**state** 重新变为0。

**4）AQS定义两种资源共享方式**

- **Exclusive**（独占）：只有一个线程能执行，如 **ReentrantLock**。又可分为**公平锁**和**非公平锁**：
  - **公平锁**：按照线程在队列中的排队顺序，先到者先拿到锁。
  - **非公平锁**：当线程要获取锁时，无视队列顺序直接去抢锁，谁抢到就是谁的，但是当无法抢到锁的时候就老老实实进入队列进行排队。也就是说非公平锁具有两次获取锁的机会。
- **Share**（共享）：多个线程可同时执行，如 **Semaphore/CountDownLatch**。

### ==10.synchronized 和 ReentrantLock 的区别==

两者的共同点：

- 都是对多线程进行**加锁**来协调多线程对**共享对象、变量**的访问
- 都是**可重入锁**，同一线程可以多次获得同一个锁（自己可以再次获取自己的内部锁）

两者的不同点（主要不同点）：

- ReentrantLock 是 **API** 级别的， synchronized 是 **JVM** 级别的；
- **底层实现**不一样， synchronized 是**同步阻塞**，使用的是**悲观并发策略**；ReentrantLock是 **同步非阻塞**，采用的是**乐观并发策略**；

次要的不同点：

- ReentrantLock 可以实现**公平锁**，所谓的公平锁就是先等待的线程先获得锁；

- ReentrantLock **显示获得、释放锁**；synchronized **隐式获得释放锁**；
- ReentrantLock **可响应中断、可轮回**；synchronized 是**不可以响应中断**的，为处理锁的不可用性提供了更高的灵活性；
- ReentrantLock 通过 **Condition** 可以绑定多个条件，线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活；

### ==11.synchronized 和 volatile 的区别==

- **volatile关键字**是线程同步的**轻量级实现**，所以**volatile性能肯定比synchronized关键字要好**。
- **volatile关键字只能用于变量而synchronized关键字可以修饰方法以及代码块**。**实际开发中使用 synchronized 关键字的场景还是更多一些**。
- 多线程访问volatile关键字**不会发生阻塞**，而synchronized关键字可能**会发生阻塞**。
- volatile关键字主要用于解决变量在多个线程之间的**可见性**，而 synchronized关键字解决的是多个线程之间访问资源的**同步性**。

### ==12.synchronized、volatile、CAS 比较==

- **synchronized** 是 **悲观锁**，属于**抢占式**，会引起其他线程**阻塞**。
- **volatile**提供多线程共享变量**可见性**和**有序性**。
- **CAS**是基于**冲突检测**的**乐观锁（非阻塞）**。

### ==13.sleep 方法和 wait 方法有什么区别?==

sleep方法和wait方法都可以用来放弃CPU一定的时间

不同点在于如果**线程持有某个对象的监视器**，sleep方法不会放弃这个对象的监视器（锁），wait方法会放弃这个**对象的监视器（锁）**。

### ==14.线程池的底层原理==

**1)线程池概念**

系统不允许无限制的创建很多线程，因为创建线程以及销毁线程非常耗费资源。

因此需要创建一个线程池，该线程池具有一定数量的线程，让其去执行各种各样的任务，当线程执行完任务之后不要销毁自己，继续去执行下一个任务。这样就避免了频繁的创建和销毁线程所带来的资源消耗。

```java
// 创建线程池，指定线程池的线程数量 corePoolSize
ExecutorService threadPool = Executors.newFixedThreadPool(10);
// 执行任务
threadPool.submit(new callable(){
    public void run(){
        
    }
})
```

**2）线程池底层原理**

**corePoolSize** + **无界阻塞队列** + 任务 + 线程 + 线程池

首先线程池初始化的时候是没有创建对应的线程的，也就是说初始情况下的线程数量时0；

然后当有任务过来的时候会首先判断线程池中的线程数量是否小于 **corePoolSize**，当小于 **corePoolSize**  的时候就新创建一个线程来执行对应的任务，当这个任务执行完毕之后就会尝试从一个 **无界阻塞队列**中获取新的任务，如果没有新的任务，此时会阻塞等待新的任务到来。

当新的任务到来时，会重复执行上面的流程，首先判断当前线程池中线程的数量是否小于 **corePoolSize**，小于的话都会直接创建一个新的线程来执行该任务，执行完了之后这个线程就尝试从 **无界阻塞队列**中获取任务去执行。这个过程一直持续到线程池中有 **corePoolSize** 个线程。

接着再次提交任务，会发现线程数量已经和 **corePoolSize** 一样大了，此时就可以直接把任务放入 **无界阻塞队列**中，线程会争抢获取任务执行，如果所有的线程都在执行任务，那么 **无界阻塞队列**中的任务就会越来越多。

### ==15.线程池的核心配置参数是干什么的？平时怎么使用？==

**1）核心配置参数介绍**

以 newFixedThreadPool(10) 为例

创建该线程池的类是 **ThreadPoolExecutor**：

```java
public ThreadPoolExecutor(int corePoolSize,
                              int maximumPoolSize,
                              long keepAliveTime,
                              TimeUnit unit,
                              BlockingQueue<Runnable> workQueue)
```

核心配置参数：

- **corePoolSize**：线程数量；
- **maximumPoolSize**：最大线程数量；
- **keepAliveTime**：额外线程空闲存活时间，额外线程是指 **maximumPoolSize - corePoolSize** 的线程；
- **workQueue**：任务队列。

线程池会按照上面介绍的实现原理根据任务来创建对应的 **corePoolSize** 数量的线程来执行任务，但是假如自定义了**workQueue**的长度，当队列满的话就会继续创建额外的线程，额外线程的数量是由 **maximumPoolSize** 参数决定的。当创建了额外线程将任务队列中的任务都执行完毕之后额外线程就属于空闲状态了，假如空闲时间超过 **keepAliveTime** 的话就会销毁掉这些额外线程，继续使用 **corePoolSize** 数量的线程来执行任务。

如果额外线程都创建完了去执行任务，但是队列还是满的，此时还有新的任务过来怎么办？

只能 reject 掉，有几种 reject 策略，可以传入 RejectedExecutionhandler

- AbortPolicy：**抛异常**
- DiscardPolicy：**抛弃任务**
- DiscardOldPolicy：**抛弃最旧任务**
- 自定义等

**2）日常使用策略**

一般比较常用的 fixed 线程池。corePoolSize 和 maximumPoolSize 数量一样，队列采用无界队列。

或者自定义线程池，按照需求来进行定义。

### ==16.如果在线程池中使用无界阻塞队列会发生什么问题？==

如果在线程池中使用无界队列的话，当线程执行任务出现故障导致任务加入的速度超过任务处理的速度的话就会使得队列中的任务越来越多，直到将内存装满，出现 **OOM** 错误。

因此使用有界队列就可以避免内存溢出。

### ==17.线程池的队列满了之后会发生什么问题？==

当队列满的话并且设置了**maximumPoolSize**为 Integer.MAX_VALUE 的话，那么会无限制的创建额外的线程出来，每个线程都有自己的栈内存，占用一定的内存资源，这样会导致内存资源耗尽，系统也会崩溃掉。即使内存没有崩溃，也会导致机器的 CPU 负载特别高。

因此可以设置**maximumPoolSize**来控制额外线程的创建数量，但这样会出现 reject 的情况，因此可以自定义一个 reject 策略，如果线程池无法执行更多的任务了，此时可以把这些任务信息持久化到磁盘中去，后台专门启动一个线程，后续等待线程池的工作负载降低了，可以慢慢的从磁盘中读取之前持久化的任务，重新提交到线程池中去执行。

### ==18.如何线上机器突然宕机，线程池的阻塞队列中的请求怎么办？==

如果线上机器突然宕机，必然会导致线程池里积压的任务丢失。

如果你要提交一个任务到线程池中，那么在提交之前，可以先在**数据库**中插入任务的信息，并更新对应的状态：未提交、已提交、已完成。提交成功之后可以更新他的状态为以提交状态。

这样当系统重启之后，后台线程去扫描数据库里的未提交和已提交状态的任务，将任务读取出来重新提交到线程池中去，继续完成任务。

### 19.线程池关闭方法

ExecutorService关于关闭主要有如下几个方法

- `shutdown`：在线程池队列中的提交的任务会执行，无法提交新的任务，注意调用这个方法，线程池不会等待（`wait`）在执行的任务执行完成，可以使用`awaitTermination`实现这个目的。这里需要注意的是：**在执行的任务因为是异步线程执行的，任务还是会继续执行，只是说线程池不会阻塞等待任务执行完成**
- `List<Runnable> shutdownNow()`：试图关闭正在执行的任务，不会执行已经提交到队列但是还没有执行的任务，返回等待执行的任务列表，同时此方法不会等待那些正在执行的任务执行完，等待执行的任务会从线程池队列移除。
- `isShutdown`：线程池是否关闭
- `isTerminated` ：判断线程池关闭后所有的任务是否都执行完了，注意这个方法只有在 `shutdown`或`shutdownNow`方法调用后才有效
- `awaitTermination`：阻塞，直到一下任务情况出现：
  - `shutdown`调用后所有任务执行完成
  - 超时
  - 当前线程中断

正常执行 shutdown 关闭线程池的时候，无论是已经在执行的任务还是提交了但未执行的任务还是会继续执行，如果某个任务执行时间很长或者阻塞了，这样会导致我们的应用程序无法关闭，有的时候这可能不是我们想要的结果。

正确的关闭流程：

- 调用`shutdown`禁止提交新的任务
- 调用 `awaitTermination`等待任务执行完成
- 调用`shutdownNow`强制关闭那些执行任务过长（可能无法正常停止）的任务

### ==20.ConcurrentHashMap==

hashmap在并发场景下会出现以下的问题：

- **死循环**的问题：HashMap 扩容的时候会调用 `resize()` 方法，多线程并发执行扩容 resize 操作，这样多个线程中维护的 entry数组下的冲突链表会由于并发的原因相互指向，形成一个**环形链表**；这样当数组该位置 get 寻找对应的key时，就发生了**死循环**。该问题在1.8中使用**尾插法**已经解决。所以在并发的情况，发生**扩容**时，可能会产生**循环链表**，在执行**get**的时候，会触发**死循环**，引起CPU的100%问题，所以一定要避免在并发环境下使用HashMap。

- JDK1.8和之前版本**多线程**操作HashMap都会在**put**的时候导致**数据不一致**或者**丢失和覆盖**。

ConcurrenthashMap是不允许保存null键和null值的。

在并发环境下建议使用 ConcurrentHashMap，这是因为 ConcurrentHashMap 是线程安全的HashMap。

**1）底层数据结构**： 

JDK1.7的 ConcurrentHashMap 底层采用 **分段的数组+链表** 实现

JDK1.8 采用的数据结构跟 HashMap1.8的结构一样，**数组+链表/红黑二叉树**。

**2）实现线程安全的方式**

在**JDK1.7**的时候，**ConcurrentHashMap（分段锁）** 对整个桶数组进行了 **分割分段(Segment)**，每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。（默认分配16个Segment） 

JDK1.7的ConcurrentHashMap：

![28](/Users/wx/project/interview/docs/秘籍/images/28.png)

1.7 已经解决了并发问题，并且能支持 N 个 Segment 这么多次数的并发，但依然存在 HashMap 在 1.7 版本中的问题。

> 那就是查询遍历链表效率太低。

因此 1.8 做了一些数据结构上的调整。

到了 **JDK1.8** 的时候已经摒弃了Segment 的概念，而是直接用 **Node 数组+链表+红黑树**的数据结构来实现，并发控制使用 **synchronized 和 CAS** 来操作。

对于 Entry 数组来说，首先对每个元素进行 **CAS**，如果该节点没有元素即为 null 则直接进行插入，结果不为 null 则证明有线程正在使用，此时使用 **synchronized** 对数组元素进行加锁，synchronized只锁定**当前链表或红黑二叉树的首节点**，这样**只要hash不冲突，就不会产生并发**；

JDK1.8的ConcurrentHashMap：

![29](/Users/wx/project/interview/docs/秘籍/images/29.png)



### ==21.ThreadLocal介绍==

ThreadLocal是一个关于**创建线程局部变量**的类。

通常情况下，我们创建的**变量**是可以被任何一个线程访问并修改的。而使用ThreadLocal创建的**变量只能被当前线程访问，其他线程则无法访问和修改**。

ThreadLocal修饰的对象存放在**堆**上，通过一些机制进行了**掩饰**。

使用起来都是在线程的**本地工作内存**中操作，并且提供了 **set** 和 **get** 方法来访问拷贝过来的变量副本。底层也是封装了**ThreadLocalMap** 集合类来绑定当前线程和变量副本的关系，各个线程独立并且访问安全！

ThreadLocal的设计思想：

**(1) ThreadLocal仅仅是个变量访问的入口；**

**(2) 每一个Thread对象都有一个ThreadLocalMap对象，这个ThreadLocalMap持有对象的引用；**

**(3) ThreadLocalMap以当前的threadLocal对象为key，以真正的存储对象为value。get()方法时通过threadLocal实例就可以找到绑定在当前线程上的副本对象。**

ThreadLocal这样设计有两个目的：

第一：可以保证当前线程结束时，相关对象可以立即被回收；

第二：ThreadLocalMap元素会大大减少，因为Map过大容易造成哈希冲突而导致性能降低。

 ThreadLocal对象通常用于防止对可变的单实例变量或全局变量进行共享。

例如：由于JDBC的连接对象不是线程安全的，因此，当多个线程应用程序在没有协同的情况下，使用全局变量时，就是线程不安全的。通过将JDBC的连接对象保存到ThreadLocal中，每个线程都会拥有自己的连接对象副本。

ThreadLocal在Spring的事物管理，包括Hibernate管理等都有出现，在web开发中，有事会用来管理用户回话HttpSession，web交互这种典型的一请求一线程的场景似乎比较适合使用ThreadLocal，但是需要注意的是，***\*由于此时session与线程关联，而Tomcat这些web服务器多采用线程池机制，也就是说线程是可以复用的，所以在每次进入的时候都需要重新进行set操作，或者使用完毕以后及时remove掉！\****

### ==22.原子类实现原理==

底层实现是通过 CAS 机制实现的。原子类的**自增、加法**等操作底层都是通过**自旋CAS**操作实现的。

### 23.多线程任务调度器

ScheduledThreadPoolExecutor 进行周期性任务的调度。

在这之前的实现需要依靠Timer和TimerTask或者其它第三方工具来完成。但Timer有不少的缺陷：

- Timer是单线程模式；
- 如果在执行任务期间某个TimerTask耗时较久，那么就会影响其它任务的调度；
- Timer的任务调度是基于绝对时间的，对系统时间敏感；
- Timer不会捕获执行TimerTask时所抛出的异常，由于Timer是单线程，所以一旦出现异常，则线程就会终止，其他任务也得不到执行。

ScheduledThreadPoolExecutor继承ThreadPoolExecutor来重用线程池的功能，它的实现方式如下：

- 将任务封装成ScheduledFutureTask对象，ScheduledFutureTask基于相对时间，不受系统时间的改变所影响；
- ScheduledFutureTask实现了`java.lang.Comparable`接口和`java.util.concurrent.Delayed`接口，所以有两个重要的方法：compareTo和getDelay。compareTo方法用于比较任务之间的优先级关系，如果距离下次执行的时间间隔较短，则优先级高；getDelay方法用于返回距离下次任务执行时间的时间间隔；
- ScheduledThreadPoolExecutor定义了一个DelayedWorkQueue，它是一个有序队列，会通过每个任务按照距离下次执行时间间隔的大小来排序；
- ScheduledFutureTask继承自FutureTask，可以通过返回Future对象来获取执行的结果。

通过如上的介绍，可以对比一下Timer和ScheduledThreadPoolExecutor：

| Timer                                            | ScheduledThreadPoolExecutor            |
| ------------------------------------------------ | -------------------------------------- |
| 单线程                                           | 多线程                                 |
| 单个任务执行时间影响其他任务调度                 | 多线程，不会影响                       |
| 基于绝对时间                                     | 基于相对时间                           |
| 一旦执行任务出现异常不会捕获，其他任务得不到执行 | 多线程，单个任务的执行不会影响其他线程 |

