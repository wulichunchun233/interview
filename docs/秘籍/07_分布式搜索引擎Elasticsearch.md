## 分布式搜索引擎 Elasticsearch

一般业内的标准都是使用分布式搜索引擎 Elasticsearch，简称 es。在鑫课堂项目中就使用到了 es 来进行课程信息的检索。

### ==0.es的基础知识==

Elasticsearch 是一个基于[Lucene](https://baike.baidu.com/item/Lucene/6753302)的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。

ES 的**索引库**是一个逻辑概念，它包括了**分词列表**及**文档列表**，同一个索引库中存储了相同类型的文档。索引库就相当于 MySQL中的表，或相当于 Mongodb 中的集合。

下图是 Elasticsearch 的索引结构，下边黑色部分是物理结构，上边黄色部分是逻辑结构，逻辑结构也是为了更好的去描述 Elasticsearch 的工作原理及去使用物理结构中的索引文件。

![1](https://wangxin1248.github.io/assets/images/2020/2020-04/1.png)

逻辑结构部分是一个**倒排索引表**：

- 将要搜索的文档内容**分词**，所有不重复的词组成**分词列表**。
- 将搜索的文档最终以**Document**方式存储起来。
- 每个词和docment都有关联。

所谓**倒排索引表**是正排索引表的相反操作，在正排索引表当中，索引查找的顺序在先找 document 文档，然后再从 document 文档中查找对应的 term 词；而倒排索引表刚好相反，索引先找 term 词，再根据 term 找对应的 document。

因此倒排索引表需要专门保存 **term 和 document 之间的对应关系**。

### ==1.es的分布式架构原理能说一下吗？（es是如何实现分布式的？）==

elasticsearch 的设计理念是分布式搜索引擎，底层其实还是基于 lucene 的。

核心思想就是**在多台机器上启动多个 es 进程实例，组成一个 es 集群**。

es 中存储的基本单位是**索引**，索引就相当于 mysql 中的一张表。

es 中的基本概念：**index、mapping、document、field**。（es在9.0版本之后忽略type）mapping就是相当于 mysql 中的表结构定义，定义了type中每个字段的名称以及字段类型以及相应的配置；往 index 中写入的一条数据就是一条 document，一条 document 就相当于 mysql 中某个表的一行，每个 document 中有多个 field，每个 field 代表了这个 document 中一个字段的值。

创建在 es 中的 **index 会被拆分为多个 shard，每个 shard 中会存放 index 中的一部分数据**。接着 shard 中的数据还会进行多个备份，也就是说每个 shard 中都有一个 **primary shard**，负责**写入数据**，但是还有几个 **replica shard**（一般 primary 和 replica 不在同一台设备上）。primary shard 写入数据之后会将数据同步到其他几个 replica shard 上去。但是客户端进行数据的读取的时候是可以通过 primary shard 或者 replica shard 任何一个 shard 都可以的。也就是说**写入只可以是 primary shard，而读取 primary 和 replica 都是可以的**。

整个 es 集群会自动选举一个节点作为 **master** 节点，这个 **master 节点其实就是干一些管理的工作**，比如维护**元数据**以及负责**切换 primary shard 和 replica shard 的身份**等。假如 master 节点宕机了则会重新选举一个节点为 master 节点。

节点宕机之后其上面的 primary shard 的身份会有对应的 replica shard 来代替成为新的 primary shard。当宕机的机器修复好之后 mater 节点会将缺失的 replica shard 分配过去，同步后续修改的数据，让集群恢复正常。

以上就是 es 的整体分布式架构原理。

### ==2.es写入和查询数据的原理是什么？==

**1）es写数据原理：**

1、客户端随机选择一个 **node** 发送写数据请求，这个 **node** 就被作为 **coordinating node**（协调节点）

2、**coordinating node** 对 **document** 进行 **hash** 判断其所要保存的 **shard** ，然后执行路由操作将其转发到具有该 **shard** 的 **primary shard** 的 **node** 上。

3、具有 **primary shard** 的 **node** 上的 **shard** 对数据进行写入操作，并将数据同步到 **replica shard** 上去。

4、**coordinating node** 发现 **primary** 和 **replica shard** 都完成数据保存操作之后会返回对应的响应信息给客户端。

**2）es读数据原理：**

查询，GET一条数据，写入了一个 **document**，这个 **document** 会自动分配一个全局唯一的 id，**doc id**，同时也是根据 **doc id** 进行 **hash** 路由到对应的 **primary shard** 上。

然后就可以通过 **doc id** 来进行查询，**node** 会根据 **doc id** 进行 **hash**，判断出来当时把 **doc id** 分配到那个 **shard** 上就从那个 **shard** 上去查询。

1、客户端随机选择一个 **node** 发送读取数据请求，该 **node** 作为 **coordinating node**

2、**coordinating node** 对 **document** 进行 **hash** 选择保存数据的 **shard**，然后将消息转发到具有该 **sahrd** 的 **node** 上，此时会使用 **round-robin 随机轮询算法**在 **primary** 和 **replica** 中随机选择一个让读请求进行负载均衡。

3、执行该请求的 **node** 会返回对应的 **document** 给 **coordinating node**

4、**coordinating node** 返回 **document** 给客户端。 

**3）es索引数据原理：**

es最强大的就是做**全文检索**

1、客户端随机选择一个 **node** 发送索引数据请求，该 **node** 作为 **coordinating node**协调节点

2、**coordinating node** 将搜索请求转发到所有的 **shard** 对应的 **primary sahrd** 或者 **replica shard **，接下来分为两步来执行

3、**query phase**：每个shard将自己的搜索结果（**doc id**）返回给协调节点，由协调节点进行数据的**合并、排序、分页**等操作，产生最终的结果

4、**fetch phase**：接着由协调节点根据 **doc id** 去各个节点上拉取实际的 **document 数据**，最后返回给客户端。

### ==3.es在数据量很大（几亿级别）的情况下如何提高查询效率？==

在海量数据的场景下，如何提升 es 的性能。（**主要就是 es 的底层实现原理的考查，针对底层实现原理来优化其中最重要的 filesystem cache**）

**1）性能优化的杀手锏 - filesystem cache**

往es写入的数据实际都写入到磁盘文件中，**磁盘文件里的数据操作系统会自动将其缓存到 os cache **里面去。也就是真正的数据保存在磁盘文件中。

然后当有客户端来读取索引的时候，会首先通过 es 的 shard 来向操作系统来读取数据，这时其实会**首先先访问操作系统的 filesystem cache，当 cache 中没有存在这些数据的时候才会去从磁盘文件中去读** ，此时就非常浪费时间了。

由此可见，es 的搜索引擎严重依赖于底层的 filesystem cache，因此如果给 filesystem cache 更多的内存，尽量让内存可以容纳所有的 index segment file 索引数据文件，那么你搜索的时候就基本都是走内存的，性能会非常高。

因此想让 es 的性能要好，最佳情况下**要保证机器的内存可以至少容纳总数据量的一半**，也就是说仅仅在 es 上存储少量的数据。同时也可以尽可能的让 es 存储比较重要的数据，将一些比较重要的数据用来检索的数据存储在es，提高效率。

一般是**使用 es + hbase 架构**。hbase的特点是适用于海量数据的在线存储，存储在 hbase 中的数据不需要做复杂的搜索，只需要进行简单的根据 id 或者范围来进行查询即可。这样使用 es 来查询到 doc id，然后使用 doc id 来去 hbase 中去查询每个 doc id 对应的数据。或者放在 mysql 里也是可以的。

一般来说**写入的数据应该小于 filesystem cache 的内存容量，或者略微大于**。

**2）缓存预热**

加入在 es 中保存的数据还是超过了 filesystem cache 的大小。这时候还可以**通过数据预热的机制来将一些比较热门的数据进行数据预处理**。也就是自己开发一个数据预热系统，然后定期的将一些比较热门的数据进行读取，这样就会将这些比较热门的数据刷入到 filesystem cache 中，也可以做到提高数据检索的效果。

**3）冷热分离**

可以将索引数据中比较常用的数据和使用较少的数据分开来存储到不同的 index 中。然后保持热数据也就**经常查询的数据全都存放在 filesystem cache 中，而那么冷数据放在磁盘中也是无所谓的**。

还可以建立两个es集群，一个作为热数据集群，存储当前小范围的热点数据；一个作为冷数据集群，存储全部的数据。

同时备集群增加一键降级到主集群的功能，两个集群地位同等重要，但都可以各自降级到另一个集群。双写策略也优化为：假设有AB集群，正常同步方式写主（A集群）异步方式写备（B集群）。A集群发生异常时，同步写B集群（主），异步写A集群（备）。

**4）document模型设计**

为了保证检索的速度，可以将需要检索的数据格式在写入的时候就直接创建好，然后直接进行检索。不要将类似于表关联之类的操作放在检索的过程中。

**5）分页检索**

ES集群的分页查询支持from和size参数，查询的时候，每个分片必须构造一个长度为from+size的优先队列，然后回传到网关节点，网关节点再对这些优先队列进行排序找到正确的size个文档。

es 的分页检索的实现原理是将需要分页的数据的前面所有的数据从每一个 shard 中取出来，然后在协调节点中进行相关分页的实现。也就是说**分页的页数越深的话需要从其他 shard 上读取的数据就越多，也就导致效率越低**。

可以的话尽量避免在系统中提供比较深的分页的功能。或者直接提示用户较深的分页是需要更多的时间来完成的。

另外的话可以像一个 APP 一样（微博）使用 scroll api 来将所有的数据建立快照保存，然后一点一点的滑动，这样就不会出现跳页的情况。性能也会有所改善。

### 4.es数据同步的方案

**MySQL数据同步到ES中，大致总结可以分为两种方案：**

- 方案1：监听MySQL的Binlog，分析Binlog将数据同步到ES集群中。
- 方案2：直接通过ES API将数据写入到ES集群中。

监听MySQL的Binlog相当于**异步**同步数据，具有一定的延时性。

在鑫课堂项目中使用 Logstash（logstash是ES下的一款开源软件，它能够同时 从多个来源采集数据、转换数据）将MySQL中 的课程信息读取到ES中创建索引，使用IK分词器进行分词。

### ==5.es中索引的数据结构==

对于索引来说（主要是倒排索引）的字典来说，有**跳跃表，B+树，前缀树，后缀树，自动状态机，哈希表**这么几种数据结构

介绍跳表之前先回顾下**二分查找**，二分查找是一种比较基础常见的查找方法，执行二分查找的前提是要有一个**有序数组**，注意：有序且要是数组。此算法可以将查找耗时由线性提升到对数时间范围。说二分查找基础是因为在它理解上可以演变出较高级的实现算法，比如跳表、二叉查找树。

为了能够快速查找docid，lucene采用了**SkipList**这一数据结构；跳跃表的实现和redis当中的一致。

