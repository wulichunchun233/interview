# ML 速成笔记

本笔记是根据 Google 提供的《机器学习速成课程》所编写的，其中记录一些机器学习中比较重要的内容，方便以后的面试过程。

课程地址：[https://developers.google.cn/machine-learning/crash-course/](https://developers.google.cn/machine-learning/crash-course/)

---
目录:

- [一、机器学习概念](#一机器学习概念)
    - [1.基本概念](#1基本概念)
    - [2.线性回归](#2线性回归)
    - [3.训练与损失](#3训练与损失)
    - [4.降低损失](#4降低损失)
    - [5.TensorFlow](#5TensorFlow)
- [二、机器学习工程](#二机器学习工程)
- [三、机器学习应用示例](# 三机器学习应用示例)
- [四、总结](#四总结)
---

## 一、机器学习概念

本文所提到的机器学习是指监督式机器学习，什么是（监督式）机器学习？简单来说，它的定义如下：

- 机器学习系统通过学习如何组合输入信息来对从未见过的数据做出有用的预测。

### 1.基本概念

下面我们来了解一下机器学习的基本术语。

#### 标签

标签是我们要预测的事物，即简单线性回归中的 y 变量。标签可以是小麦未来的价格、图片中显示的动物品种、音频剪辑的含义或任何事物。

#### 特征

特征是输入变量，即简单线性回归中的 x 变量。简单的机器学习项目可能会使用单个特征，而比较复杂的机器学习项目可能会使用数百万个特征，按如下方式指定：

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mspace linebreak="newline" />
  <mrow class="MJX-TeXAtom-ORD">
    <msub>
      <mi>x</mi>
      <mn>1</mn>
    </msub>
    <mo>,</mo>
    <msub>
      <mi>x</mi>
      <mn>2</mn>
    </msub>
    <mo>,</mo>
    <mo>.</mo>
    <mo>.</mo>
    <mo>.</mo>
    <msub>
      <mi>x</mi>
      <mi>N</mi>
    </msub>
    <mspace linebreak="newline" />
  </mrow>
</math>


#### 样本

样本是指数据的特定实例：x。（我们采用粗体 x 表示它是一个矢量。）我们将样本分为以下两类：
- 有标签样本
- 无标签样本

有标签样本同时包含特征和标签。即：

```
labeled examples: {features, label}: (x, y)
```

我们使用有标签样本来训练模型。

#### 模型

模型定义了特征与标签之间的关系。我们来重点介绍一下模型生命周期的两个阶段：

- **训练**是指创建或学习模型。也就是说，向模型展示有标签样本，让模型逐渐学习特征与标签之间的关系。

- **推断**是指将训练后的模型应用于无标签样本。也就是说，使用经过训练的模型做出有用的预测 (y')。例如，在推断期间，您可以针对新的无标签样本预测 medianHouseValue。

#### 回归与分类

回归模型可预测**连续值**。例如，回归模型做出的预测可回答如下问题：

- 加利福尼亚州一栋房产的价值是多少？

- 用户点击此广告的概率是多少？

分类模型可预测**离散值**。例如，分类模型做出的预测可回答如下问题：

- 某个指定电子邮件是垃圾邮件还是非垃圾邮件？

- 这是一张狗、猫还是仓鼠图片？

### 2.线性回归

一个简单的线性回归的方程如下所示：

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <msup>
    <mi>y</mi>
    <mo>&#x2032;</mo>
  </msup>
  <mo>=</mo>
  <mi>b</mi>
  <mo>+</mo>
  <msub>
    <mi>w</mi>
    <mn>1</mn>
  </msub>
  <msub>
    <mi>x</mi>
    <mn>1</mn>
  </msub>
</math>

其中：

- y'指的是预测标签（理想输出值）。
- b指的是偏差（y 轴截距）。而在一些机器学习文档中，它称为 w0。
- w1指的是特征 1 的权重。
- x1指的是特征（已知输入项）。


下标预示着可以用多个特征来表示更复杂的模型。例如，具有三个特征的模型可以采用以下方程式：

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <msup>
    <mi>y</mi>
    <mo>&#x2032;</mo>
  </msup>
  <mo>=</mo>
  <mi>b</mi>
  <mo>+</mo>
  <msub>
    <mi>w</mi>
    <mn>1</mn>
  </msub>
  <msub>
    <mi>x</mi>
    <mn>1</mn>
  </msub>
  <mo>+</mo>
  <msub>
    <mi>w</mi>
    <mn>2</mn>
  </msub>
  <msub>
    <mi>x</mi>
    <mn>2</mn>
  </msub>
  <mo>+</mo>
  <msub>
    <mi>w</mi>
    <mn>3</mn>
  </msub>
  <msub>
    <mi>x</mi>
    <mn>3</mn>
  </msub>
</math>

### 3.训练与损失

简单来说，**训练**模型表示通过有标签样本来学习（确定）所有权重和偏差的理想值。

在监督式学习中，机器学习算法通过以下方式构建模型：

    检查多个样本并尝试找出可最大限度地减少损失的模型
    
这一过程称为经验**风险最小化**。

损失是对糟糕预测的惩罚。也就是说，损失是一个数值，表示对于单个样本而言模型预测的准确程度。如果模型的预测完全准确，则损失为零，否则损失会较大。

训练模型的目标是从所有样本中找到一组平均损失“较小”的权重和偏差。例如，下图左侧显示的是损失较大的模型，右侧显示的是损失较小的模型。关于此图，请注意以下几点：

- 红色箭头表示损失。
- 蓝线表示预测。

![01_LossSideBySide](/assets/images/ml/01_LossSideBySide.png)

请注意，左侧曲线图中的红色箭头比右侧曲线图中的对应红色箭头长得多。显然，相较于左侧曲线图中的蓝线，右侧曲线图中的蓝线代表的是预测效果更好的模型。

对于评价模型的损失来说，有一下两种方式来评估：

1. 平方损失

平方损失：一种常见的损失函数

接下来我们要看的线性回归模型使用的是一种称为平方损失（又称为 L2 损失）的损失函数。单个样本的平方损失如下：

```
= the square of the difference between the label and the prediction
= (observation - prediction(x))2
= (y - y')2
```

2.均方误差

均方误差 (MSE) 指的是每个样本的平均平方损失。

要计算 MSE，请求出各个样本的所有平方损失之和，然后除以样本数量：
![02](/assets/images/ml/02.png)

其中：

- (x,y)指的是样本，其中
    - x指的是模型进行预测时使用的特征集（例如，温度、年龄和交配成功率）。
    - y指的是样本的标签（例如，每分钟的鸣叫次数）。
- prediction(x)指的是权重和偏差与特征集 x 结合的函数。
- D指的是包含多个有标签样本（即 (x,y)）的数据集。
- N 指的是 D 中的样本数量。

虽然 MSE 常用于机器学习，但它既不是唯一实用的损失函数，也不是适用于所有情形的最佳损失函数。

### 4.降低损失

#### 迭代法

下图显示了机器学习算法用于训练模型的迭代试错过程：

![03_GradientDescentDiagram](/assets/images/ml/03_GradientDescentDiagram.svg)

我们将在整个机器学习速成课程中使用相同的迭代方法详细说明各种复杂情况。迭代策略在机器学习中的应用非常普遍，这主要是因为它们可以很好地扩展到大型数据集。

“模型”部分将一个或多个特征作为输入，然后返回一个预测 (y') 作为输出。

为了进行简化，不妨考虑一种采用一个特征并返回一个预测的模型：

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <msup>
    <mi>y</mi>
    <mo>&#x2032;</mo>
  </msup>
  <mo>=</mo>
  <mi>b</mi>
  <mo>+</mo>
  <msub>
    <mi>w</mi>
    <mn>1</mn>
  </msub>
  <msub>
    <mi>x</mi>
    <mn>1</mn>
  </msub>
</math>

最后，我们来看图的“计算参数更新”部分。机器学习系统就是在此部分检查损失函数的值，并为 b 和 w1 生成新值。

现在，假设这个神秘的绿色框会产生新值，然后机器学习系统将根据所有标签重新评估所有特征，为损失函数生成一个新值，而该值又产生新的参数值。这种学习过程会持续迭代，直到该算法发现损失可能最低的模型参数。通常，您可以不断迭代，直到总体损失不再变化或至少变化极其缓慢为止。这时候，我们可以说该模型已收敛。

注意：
在训练机器学习模型时，首先对权重和偏差进行初始猜测，然后反复调整这些猜测，直到获得损失可能最低的权重和偏差为止。

#### 梯度下降法

迭代方法图（上图）包含一个标题为“计算参数更新”的华而不实的绿框。现在，我们将用更实质的方法代替这种华而不实的算法。

假设我们有时间和计算资源来计算 w1 的所有可能值的损失。对于我们一直在研究的回归问题，所产生的损失与 w1 的图形始终是凸形。换言之，图形始终是碗状图，如下所示：

![04_convex](/assets/images/ml/04_convex.svg)

凸形问题只有一个最低点；即只存在一个斜率正好为 0 的位置。这个最小值就是损失函数收敛之处。

通过计算整个数据集中 w1 每个可能值的损失函数来找到收敛点这种方法效率太低。我们来研究一种更好的机制，这种机制在机器学习领域非常热门，称为梯度下降法。

梯度下降法的第一个阶段是为 w1 选择一个起始值（起点）。起点并不重要；因此很多算法就直接将 w1 设为 0 或随机选择一个值。下图显示的是我们选择了一个稍大于 0 的起点：

![05_GradientDescentStartingPoint](/assets/images/ml/05_GradientDescentStartingPoint.svg)

然后，梯度下降法算法会计算损失曲线在起点处的梯度。简而言之，梯度是偏导数的矢量；它可以让您了解哪个方向距离目标“更近”或“更远”。请注意，损失相对于单个权重的梯度就等于导数。

请注意，梯度是一个矢量，因此具有以下两个特征：

- 方向
- 大小

梯度始终指向损失函数中增长最为迅猛的方向。梯度下降法算法会沿着负梯度的方向走一步，以便尽快降低损失。

![06_GradientDescentNegativeGradient](/assets/images/ml/06_GradientDescentNegativeGradient.svg)

为了确定损失函数曲线上的下一个点，梯度下降法算法会将梯度大小的一部分与起点相加，如下图所示：

![07_GradientDescentGradientStep](/assets/images/ml/07_GradientDescentGradientStep.svg)

然后，梯度下降法会重复此过程，逐渐接近最低点。

#### 学习速率

正如之前所述，梯度矢量具有方向和大小。

梯度下降法算法用梯度乘以一个称为学习速率（有时也称为步长）的标量，以确定下一个点的位置。例如，如果梯度大小为 2.5，学习速率为 0.01，则梯度下降法算法会选择距离前一个点 0.025 的位置作为下一个点。

超参数是编程人员在机器学习算法中用于调整的旋钮。大多数机器学习编程人员会花费相当多的时间来调整学习速率。如果您选择的学习速率过小，就会花费太长的学习时间：

![08_LearningRateTooSmall](/assets/images/ml/08_LearningRateTooSmall.svg)

相反，如果指定的学习速率过大，下一个点将永远在 U 形曲线的底部随意弹跳，就好像量子力学实验出现了严重错误一样：

![09_LearningRateTooLarge](/assets/images/ml/09_LearningRateTooLarge.svg)

每个回归问题都存在一个**金发姑娘**学习速率。“金发姑娘”值与损失函数的平坦程度相关。如果您知道损失函数的梯度较小，则可以放心地试着采用更大的学习速率，以补偿较小的梯度并获得更大的步长。

![10_LearningRateJustRight](/assets/images/ml/10_LearningRateJustRight.svg)

理想的学习速率：

- 一维空间中的理想学习速率是f(x) 对 x 的二阶导数的倒数。
- 二维或多维空间中的理想学习速率是海森矩阵（由二阶偏导数组成的矩阵）的倒数。
- 广义凸函数的情况则更为复杂。

#### 随机梯度下降法

在梯度下降法中，批量指的是用于在单次迭代中计算梯度的样本总数。到目前为止，一直假定批量是指整个数据集。就 Google 的规模而言，数据集通常包含数十亿甚至数千亿个样本。此外，Google 数据集通常包含海量特征。因此，一个批量可能相当巨大。如果是超大批量，则单次迭代就可能要花费很长时间进行计算。

包含随机抽样样本的大型数据集可能包含冗余数据。实际上，批量大小越大，出现冗余的可能性就越高。一些冗余可能有助于消除杂乱的梯度，但超大批量所具备的预测价值往往并不比大型批量高。

如果我们可以通过更少的计算量得出正确的平均梯度，会怎么样？通过从我们的数据集中随机选择样本，我们可以通过小得多的数据集估算（尽管过程非常杂乱）出较大的平均值。 

**随机梯度下降法 (SGD)** 将这种想法运用到极致，它每次迭代只使用一个样本（批量大小为 1）。如果进行足够的迭代，SGD 也可以发挥作用，但过程会非常杂乱。“随机”这一术语表示构成各个批量的一个样本都是随机选择的。

**小批量随机梯度下降法(小批量 SGD)** 是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。

为了简化说明，只针对单个特征重点介绍了梯度下降法。而梯度下降法也适用于包含多个特征的特征集。

### 5.TensorFlow

#### TensorFlow工具包

下图显示了 TensorFlow 工具包的当前层次结构：

![11_TFHierarchy](/assets/images/ml/11_TFHierarchy.svg)

下表总结了不同层的用途：

|工具包	| 说明 |
| ----- | ----- |
|Estimator (tf.estimator)	|高级 OOP API。|
|tf.layers/tf.losses/tf.metrics	|用于常见模型组件的库。|
|TensorFlow	|低级 API|

TensorFlow 由以下两个组件组成：

- 图协议缓冲区
- 执行（分布式）图的运行时

这两个组件类似于 Java 编译器和 JVM。正如 JVM 会实施在多个硬件平台（CPU 和 GPU）上一样，TensorFlow 也是如此。

##### tf.estimator API

tf.estimator 与 scikit-learn API 兼容。(scikit-learn 是极其热门的 Python 开放源代码机器学习库，拥有超过 10 万名用户，其中包括许多 Google 员工。)

以下是在 tf.estimator 中实现的线性回归模型的示例：

```python
import tensorflow as tf

# Set up a linear classifier.
classifier = tf.estimator.LinearClassifier()

# Train the model on some example data.
classifier.train(input_fn=train_input_fn, steps=2000)

# Use it to predict.
predictions = classifier.predict(input_fn=predict_input_fn)
```

#### TensorFlow 模型

##### 使用 TensorFlow 构建模型的基本步骤

- 第 1 步：定义特征并配置特征列
- 第 2 步：定义目标
- 第 3 步：配置模型
- 第 4 步：定义输入函数
- 第 5 步：训练模型
- 第 6 步：评估模型
- 额外步骤：调整模型超参数

##### 有适用于模型调整的标准启发法吗？

这是一个常见的问题。简短的答案是，不同超参数的效果取决于数据。因此，不存在必须遵循的规则，您需要对自己的数据进行测试。

即便如此，我们仍在下面列出了几条可为您提供指导的经验法则：

- 训练误差应该稳步减小，刚开始是急剧减小，最终应随着训练收敛达到平稳状态。
- 如果训练尚未收敛，尝试运行更长的时间。
- 如果训练误差减小速度过慢，则提高学习速率也许有助于加快其减小速度。
- 但有时如果学习速率过高，训练误差的减小速度反而会变慢。
- 如果训练误差变化很大，尝试降低学习速率。
- 较低的学习速率和较大的步数/较大的批量大小通常是不错的组合。
- 批量大小过小也会导致不稳定情况。不妨先尝试 100 或 1000 等较大的值，然后逐渐减小值的大小，直到出现性能降低的情况。

重申一下，切勿严格遵循这些经验法则，因为效果取决于数据。请始终进行试验和验证。

### 6.泛化(Generalization)

**过拟合**是由于模型的复杂程度超出所需程度而造成的。机器学习的基本冲突是适当拟合我们的数据，但也要尽可能简单地拟合数据。

机器学习的目标是对从真实概率分布（已隐藏）中抽取的新数据做出良好预测。

泛化边界，即统计化描述模型根据以下因素泛化到新数据的能力：

- 模型的复杂程度
- 模型在处理训练数据方面的表现

机器学习模型旨在根据以前未见过的新数据做出良好预测。但是，如果您要根据数据集构建模型，如何获得以前未见过的数据呢？一种方法是将您的数据集分成两个子集：

- 训练集 - 用于训练模型的子集。
- 测试集 - 用于测试模型的子集。

一般来说，在测试集上表现是否良好是衡量能否在新数据上表现良好的有用指标，前提是：

- 测试集足够大。
- 您不会反复使用相同的测试集来作假。

以下三项基本假设阐明了泛化：

- 我们从分布中随机抽取独立同分布 (i.i.d) 的样本。换言之，样本之间不会互相影响。（另一种解释：i.i.d. 是表示变量随机性的一种方式）。
- 分布是平稳的；即分布在数据集内不会发生变化。
- 我们从同一分布的数据划分中抽取样本。

### 7.训练集和测试集

为了保证模型训练的准确性，一般考虑将数据集分割为训练集和测试集。

![12_PartitionTwoSets](/assets/images/ml/12_PartitionTwoSets.svg)

- 训练集 - 用于训练模型的子集。
- 测试集 - 用于测试训练后模型的子集。

测试集满足以下两个条件：

- 规模足够大，可产生具有统计意义的结果。
- 能代表整个数据集。换言之，挑选的测试集的特征应该与训练集的特征相同。

并且注意：**请勿对测试数据进行训练。**

### 8.验证 (Validation)

如果按照上面的方式将数据集划分为了训练集和测试集的话，那么模型的运行方式如下图所示：

![13_WorkflowWithTestSet](/assets/images/ml/13_WorkflowWithTestSet.svg)

在图中，“调整模型”指的是调整您可以想到的关于模型的任何方面，从更改学习速率、添加或移除特征，到从头开始设计全新模型。该工作流程结束时，您可以选择在测试集上获得最佳效果的模型。

将数据集划分为两个子集是个不错的想法，但不是万能良方。通过将数据集划分为三个子集（如下图所示），您可以大幅降低过拟合的发生几率：

![14_PartitionThreeSets](/assets/images/ml/14_PartitionThreeSets.svg)

使用验证集评估训练集的效果。然后，在模型“通过”验证集之后，使用测试集再次检查评估结果。下图展示了这一新工作流程：

![15_WorkflowWithValidationSet](/assets/images/ml/15_WorkflowWithValidationSet.svg)

在这一经过改进的工作流程中：

- 选择在验证集上获得最佳效果的模型。
- 使用测试集再次检查该模型。

该工作流程之所以更好，原因在于它暴露给测试集的信息更少。

## 二、机器学习工程

## 三、机器学习应用示例

## 四、总结